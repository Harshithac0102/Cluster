{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQzdV-cQOHr0"
      },
      "outputs": [],
      "source": [
        "# Databricks notebook source\n",
        "# MAGIC %md # Distributed ABSA & Product Summary\n",
        "# MAGIC Widgets: product_id, reviews_blob_path, raw_container, raw_results_prefix, summary_prefix\n",
        "\n",
        "# COMMAND ----------\n",
        "dbutils.widgets.text(\"product_id\",\"\")\n",
        "dbutils.widgets.text(\"reviews_blob_path\",\"\")\n",
        "dbutils.widgets.text(\"raw_container\",\"\")\n",
        "dbutils.widgets.text(\"raw_results_prefix\",\"\")\n",
        "dbutils.widgets.text(\"summary_prefix\",\"\")\n",
        "\n",
        "product_id          = dbutils.widgets.get(\"product_id\")\n",
        "reviews_blob_path   = dbutils.widgets.get(\"reviews_blob_path\")\n",
        "RAW_CONTAINER       = dbutils.widgets.get(\"raw_container\")\n",
        "RAW_RESULTS_PREFIX  = dbutils.widgets.get(\"raw_results_prefix\")\n",
        "SUMMARY_PREFIX      = dbutils.widgets.get(\"summary_prefix\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# COMMAND ----------\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "from pyspark.sql.types import *\n",
        "import mlflow\n",
        "import json, re\n",
        "from gensim.summarization import summarize\n",
        "import sqlite3\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ABSA_Product_Summary\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .config(\"spark.executor.instances\", \"10\") \\\n",
        "    .config(\"spark.executor.memory\", \"8g\") \\\n",
        "    .config(\"spark.executor.cores\", \"4\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "4AIlBJH3OfAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# COMMAND ----------\n",
        "# 1. Load raw reviews JSONL from ADLS Gen2 via ABFSS\n",
        "storage_account = \"reviewsenseproject\"  # replace with your account\n",
        "raw_path = f\"abfss://{RAW_CONTAINER}@{storage_account}.dfs.core.windows.net/{reviews_blob_path}\"\n",
        "df_raw = spark.read.json(raw_path).selectExpr(\"review_id\", \"text as review_text\")"
      ],
      "metadata": {
        "id": "IaoJHG7dOd06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# COMMAND ----------\n",
        "# 2. Load models from MLflow (Azure ML Registry via MLflow Tracking URI)\n",
        "aspect_model    = mlflow.pytorch.load_model(\"models:/bert-aspect-extraction/Production\")\n",
        "sent_model      = mlflow.pytorch.load_model(\"models:/bert-sentiment-classification/Production\")\n",
        "scoring_pyfunc  = mlflow.pyfunc.load_model(\"models:/gpt2-scoring-justification/Production\")\n",
        "\n",
        "from transformers import BertTokenizer, GPT2Tokenizer\n",
        "# Tokenizer paths assumed packaged in model artifacts\n",
        "aspect_tok    = BertTokenizer.from_pretrained(\"/dbfs/models/bert-aspect-extraction/tokenizer\")\n",
        "sent_tok      = BertTokenizer.from_pretrained(\"/dbfs/models/bert-sentiment-classification/tokenizer\")\n",
        "gpt2_tok      = GPT2Tokenizer.from_pretrained(\"/dbfs/models/gpt2-scoring-justification/tokenizer\")\n",
        "\n",
        "# Broadcast\n",
        "bc_aspect   = spark.sparkContext.broadcast((aspect_model, aspect_tok))\n",
        "bc_sent     = spark.sparkContext.broadcast((sent_model,   sent_tok))\n",
        "bc_score    = spark.sparkContext.broadcast((scoring_pyfunc, gpt2_tok))\n"
      ],
      "metadata": {
        "id": "R9OF8IbUOcpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COMMAND ----------\n",
        "# 3. UDF: Per-review ABSA\n",
        "def split_sentences(text):\n",
        "    return re.split(r'(?<=[.!?]) +', text)\n",
        "\n",
        "ASPECTS = [\n",
        "    \"Product Quality\",\"Content/Performance\",\"User Experience\",\"Value for Money\",\n",
        "    \"Customer Service\",\"Aesthetics/Design\",\"Functionality/Features\",\n",
        "    \"Ease of Use/Accessibility\",\"Durability/Longevity\",\"Shipping and Packaging\"\n",
        "]\n",
        "\n",
        "def analyze_review(review_text):\n",
        "    a_model, a_tok = bc_aspect.value\n",
        "    s_model, s_tok = bc_sent.value\n",
        "    score_model, gpt2_tok = bc_score.value\n",
        "\n",
        "    # Aspect Extraction (BERT TokenClassification)\n",
        "    tokens = a_tok.tokenize(review_text)\n",
        "    ids    = a_tok.encode(review_text, return_tensors=\"pt\")\n",
        "    mask   = ids.ne(a_tok.pad_token_id).long()\n",
        "    outs   = a_model(ids, attention_mask=mask)\n",
        "    preds  = outs.logits.argmax(dim=2).squeeze().tolist()\n",
        "    tags   = [a_model.config.id2label[p] for p in preds]\n",
        "\n",
        "    extracted = []\n",
        "    cur = \"\"\n",
        "    for tok, tag in zip(tokens, tags):\n",
        "        if tag.startswith(\"B-\"):\n",
        "            if cur: extracted.append(cur.strip())\n",
        "            cur = tok.replace(\"##\",\"\")\n",
        "        elif tag.startswith(\"I-\"):\n",
        "            cur += \" \"+tok.replace(\"##\",\"\")\n",
        "        else:\n",
        "            if cur: extracted.append(cur.strip()); cur=\"\"\n",
        "    if cur: extracted.append(cur.strip())\n",
        "\n",
        "    results = []\n",
        "    for asp in ASPECTS:\n",
        "        if asp in extracted:\n",
        "            sents = [s for s in split_sentences(review_text) if asp.lower() in s.lower()]\n",
        "            scores, justs = [], []\n",
        "            for s in sents:\n",
        "                # Sentiment Classification\n",
        "                inp = f\"[CLS] {s_tok.sep_token} {asp} {s_tok.sep_token} {s} [SEP]\"\n",
        "                iids = s_tok.encode(inp, return_tensors=\"pt\")\n",
        "                msk  = iids.ne(s_tok.pad_token_id).long()\n",
        "                out  = s_model(iids, attention_mask=msk)\n",
        "                lbl  = out.logits.argmax(dim=1).item()\n",
        "                sentiment = s_model.config.id2label[lbl]\n",
        "                # Scoring & Justification via pyfunc\n",
        "                payload = json.dumps({\"aspect\":asp,\"sentiment\":sentiment,\"sentence\":s})\n",
        "                sj = score_model.predict(payload)\n",
        "                scores.append(float(sj[\"score\"]))\n",
        "                justs.append(sj[\"justification\"])\n",
        "            avg = sum(scores)/len(scores)\n",
        "            overall = \"Positive\" if avg>=20 else \"Negative\" if avg<=-20 else \"Mixed\"\n",
        "            results.append((review_text, asp, sentiment, avg, justs))\n",
        "        else:\n",
        "            results.append((review_text, asp, \"Neutral\", 0.0, []))\n",
        "    return results\n",
        "\n",
        "schema = ArrayType(\n",
        "    StructType([\n",
        "        StructField(\"review_text\", StringType()),\n",
        "        StructField(\"aspect\", StringType()),\n",
        "        StructField(\"sentiment\", StringType()),\n",
        "        StructField(\"score\", FloatType()),\n",
        "        StructField(\"justifications\", ArrayType(StringType()))\n",
        "    ])\n",
        ")\n",
        "\n",
        "analyze_udf = F.udf(analyze_review, schema)"
      ],
      "metadata": {
        "id": "zrRXS-x_ObMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COMMAND ----------\n",
        "# 4. Apply UDF in parallel across all reviews\n",
        "df_absa = df_raw.withColumn(\"analysis\", analyze_udf(F.col(\"review_text\")))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YzUu2N1QOZnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 5. Explode per-review results & write raw results to Blob\n",
        "df_flat = df_absa.select(\n",
        "    \"review_id\",\n",
        "    F.posexplode(\"analysis\").alias(\"pos\",\"item\")\n",
        ").select(\n",
        "    \"review_id\",\n",
        "    F.col(\"item.aspect\").alias(\"aspect\"),\n",
        "    F.col(\"item.sentiment\").alias(\"sentiment\"),\n",
        "    F.col(\"item.score\").alias(\"score\"),\n",
        "    F.col(\"item.justifications\").alias(\"justifications\")\n",
        ")\n",
        "\n",
        "raw_out_path = f\"abfss://{RAW_RESULTS_PREFIX}@{storage_account}.dfs.core.windows.net/{product_id}/\"\n",
        "df_flat.write.mode(\"overwrite\").parquet(raw_out_path)\n",
        "\n",
        "# COMMAND ----------\n",
        "# 6. Aggregate to productâ€‘level\n",
        "df_grouped = df_flat.groupBy(\"aspect\").agg(\n",
        "    F.avg(\"score\").alias(\"avg_score\"),\n",
        "    F.count(\"*\").alias(\"mention_count\"),\n",
        "    F.sum(F.expr(\"CASE WHEN sentiment='Positive' THEN 1 ELSE 0 END\")).alias(\"cnt_pos\"),\n",
        "    F.sum(F.expr(\"CASE WHEN sentiment='Mixed'    THEN 1 ELSE 0 END\")).alias(\"cnt_mixed\"),\n",
        "    F.sum(F.expr(\"CASE WHEN sentiment='Negative' THEN 1 ELSE 0 END\")).alias(\"cnt_neg\"),\n",
        "    F.collect_list(\"justifications\").alias(\"all_justs\")\n",
        ").withColumn(\n",
        "    \"pct_positive\", 100*F.col(\"cnt_pos\")/F.col(\"mention_count\")\n",
        ").withColumn(\n",
        "    \"pct_mixed\",    100*F.col(\"cnt_mixed\")/F.col(\"mention_count\")\n",
        ").withColumn(\n",
        "    \"pct_negative\", 100*F.col(\"cnt_neg\")/F.col(\"mention_count\")\n",
        ")\n"
      ],
      "metadata": {
        "id": "-PTMx8iUOYxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 7. Extractive summarization of justifications\n",
        "def extractive_summary(just_lists):\n",
        "    sents = [s for sub in just_lists for s in sub]\n",
        "    doc   = \"\\n\".join(sents)\n",
        "    if len(sents)<3:\n",
        "        return doc\n",
        "    try:\n",
        "        return summarize(doc, word_count=60)\n",
        "    except:\n",
        "        return doc[:300]\n",
        "\n",
        "summ_udf = F.udf(extractive_summary, StringType())\n",
        "\n",
        "df_summary = df_grouped.withColumn(\n",
        "    \"product_justification\", summ_udf(F.col(\"all_justs\"))\n",
        ").withColumn(\n",
        "    \"overall_sentiment\",\n",
        "    F.when(F.col(\"avg_score\")>=20, \"Positive\")\n",
        "     .when(F.col(\"avg_score\")<=-20, \"Negative\")\n",
        "     .otherwise(\"Mixed\")\n",
        ").select(\n",
        "    F.lit(product_id).alias(\"product_id\"),\n",
        "    \"aspect\",\"avg_score\",\"mention_count\",\n",
        "    \"pct_positive\",\"pct_mixed\",\"pct_negative\",\n",
        "    \"overall_sentiment\",\"product_justification\"\n",
        ")"
      ],
      "metadata": {
        "id": "oP_lJTLpOXWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Write summary JSONL to Blob\n",
        "summary_out = f\"abfss://{SUMMARY_PREFIX}@{storage_account}.dfs.core.windows.net/{product_id}/summary.jsonl\"\n",
        "df_summary.coalesce(1).write.mode(\"overwrite\").json(summary_out)\n"
      ],
      "metadata": {
        "id": "8tx4jJwVOV7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Store summary in local SQLite on driver\n",
        "conn = sqlite3.connect(\"/dbfs/tmp/absa_summary.db\")\n",
        "df_pd = df_summary.toPandas()\n",
        "df_pd.to_sql(\"product_summary\", conn, if_exists=\"replace\", index=False)\n",
        "conn.close()\n",
        "\n",
        "# COMMAND ----------\n",
        "print(f\"Raw perâ€‘review results at: {raw_out_path}\")\n",
        "print(f\"Product summary JSON at: {summary_out}\")\n",
        "print(\"SQLite DB stored at /dbfs/tmp/absa_summary.db\")"
      ],
      "metadata": {
        "id": "kBcx5Ro5OVIq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}